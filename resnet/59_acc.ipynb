{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_jvUfaq45FX","executionInfo":{"status":"ok","timestamp":1683375951051,"user_tz":-60,"elapsed":5342,"user":{"displayName":"Bailey Uniacke","userId":"03860064819488115762"}},"outputId":"6484a926-71d3-469b-d8da-46a1f47e4ca8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on cuda\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Running on {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDlZe3cH45Fa","executionInfo":{"status":"ok","timestamp":1683375998276,"user_tz":-60,"elapsed":43688,"user":{"displayName":"Bailey Uniacke","userId":"03860064819488115762"}},"outputId":"888e8747-fa0b-466d-cc12-4be18ac1049d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to flowers/flowers-102/102flowers.tgz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 344862509/344862509 [00:24<00:00, 14042048.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting flowers/flowers-102/102flowers.tgz to flowers/flowers-102\n","Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to flowers/flowers-102/imagelabels.mat\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 502/502 [00:00<00:00, 541687.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to flowers/flowers-102/setid.mat\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 14989/14989 [00:00<00:00, 45229081.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Number of training examples: 1020\n","Number of test examples: 6149\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["def data_loader(data, batch_size, random_seed=123, valid_size=0.1, shuffle=True,test=False):\n","    normalise = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),normalise])\n","    transform_train = transforms.Compose(\n","        [\n","        transforms.RandomRotation(5),\n","         transforms.RandomResizedCrop(224),\n","         transforms.RandomHorizontalFlip(),\n","         transforms.ToTensor(),\n","         normalise])\n","    if test:\n","        dataset = datasets.Flowers102(root=data, split=\"test\", download=True, transform=transform)\n","        data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n","        print(f\"Number of test examples: {len(dataset)}\")\n","        return data_loader\n","    train_dataset = datasets.Flowers102(root=data, split=\"train\", download=True, transform=transform_train)\n","    validation_dataset = datasets.Flowers102(root=data, split=\"val\", download=True, transform=transform)\n","    num_train = len(train_dataset)\n","    indices = list(range(num_train))\n","    split = int(np.floor(valid_size * num_train))\n","\n","    if shuffle:\n","        np.random.seed(random_seed)\n","        np.random.shuffle(indices)\n","    # train_idx, valid_idx = indices[split:], indices[:split]\n","    # train_sampler = torch.utils.data.sampler.SubsetRandomSampler(1020)\n","    # valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(1020)\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n","    valid_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=True)\n","    print(f\"Number of training examples: {len(train_dataset)}\")\n","    return (train_loader, valid_loader)\n","\n","train_loader, valid_loader = data_loader(data='flowers', batch_size=4, shuffle=True, test=False)\n","test_loader = data_loader(data='flowers', batch_size=4, shuffle=True, test=True)\n","# normalise = transforms.Normalize(mean=[0.4914,0.4822,0.4465],std=[0.2023,0.1994,0.2010])\n","# transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),normalise])\n","# transform_train = transforms.Compose(\n","#     [transforms.Resize((256,256)),\n","#      transforms.RandomHorizontalFlip(),\n","#      transforms.RandomVerticalFlip(p=0.5),\n","#      transforms.CenterCrop(224),\n","#      transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.4, hue=0.1),\n","#      transforms.RandomRotation(10),\n","# #      transforms.ToTensor(),\n","# #      normalise])\n","# train_dataset = datasets.Flowers102(root='./data', split=\"train\", download=True, transform=transform_train)\n","# validation_dataset = datasets.Flowers102(root=\"./data/\", split=\"val\", download=True, transform=transform)\n","# test = datasets.Flowers102(root=\"./data\", split=\"test\", download=True, transform=transform)\n","# train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","# valid_loader = DataLoader(dataset=validation_dataset, batch_size=32, shuffle=False)\n","# test_loader = DataLoader(dataset=test, batch_size=32, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22oD_Lr345Fc"},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(Block, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU()\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","        self.downsample = downsample\n","        self.relu = nn.ReLU()\n","        self.out_channels = out_channels\n","    \n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-ndSLZn45Fd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJpOotvg45Fe"},"outputs":[],"source":["class Rn(nn.Module):\n","    def __init__(self, block, layers, num_classes=102):\n","        super(Rn, self).__init__()\n","        self.in_planes = 64\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer0 = self._make_layer(block, 64, layers[0], stride=1)\n","        self.layer1 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer2 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer3 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512, num_classes)\n","    \n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_planes != planes:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(in_channels=self.in_planes, out_channels=planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","        layers = []\n","        layers.append(block(self.in_planes, planes, stride, downsample))\n","        self.in_planes = planes\n","        for i in range(1, blocks):\n","            layers.append(block(self.in_planes, planes))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool(x)\n","        x = self.layer0(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0),-1)\n","        x = self.fc(x)\n","        return x\n","    "]},{"cell_type":"code","source":["!pip install ray"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENMX1bgWCAo-","executionInfo":{"status":"ok","timestamp":1683376071874,"user_tz":-60,"elapsed":14909,"user":{"displayName":"Bailey Uniacke","userId":"03860064819488115762"}},"outputId":"1edf9d31-40fa-4c03-c546-26d089da571d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ray\n","  Downloading ray-2.4.0-cp310-cp310-manylinux2014_x86_64.whl (58.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.12.0)\n","Collecting virtualenv<20.21.1,>=20.0.24\n","  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.27.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray) (23.1.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.3)\n","Collecting frozenlist\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray) (1.22.4)\n","Collecting grpcio<=1.51.3,>=1.42.0\n","  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.3.3)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n","Collecting aiosignal\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (23.1)\n","Collecting distlib<1,>=0.3.6\n","  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray) (3.3.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.19.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.15)\n","Installing collected packages: distlib, virtualenv, grpcio, frozenlist, aiosignal, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.54.0\n","    Uninstalling grpcio-1.54.0:\n","      Successfully uninstalled grpcio-1.54.0\n","Successfully installed aiosignal-1.3.1 distlib-0.3.6 frozenlist-1.3.3 grpcio-1.51.3 ray-2.4.0 virtualenv-20.21.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUTKqfMM45Ff"},"outputs":[],"source":["# num_classes = 102\n","# num_epochs = 300\n","# learning_rate = 0.0005\n","# weight_decay = 5e-4\n","# print_every = 5\n","# model = Rn(Block, [2,2,2,2], num_classes).to(device)\n","# criterion = nn.CrossEntropyLoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n","# print(train_loader)\n","# total_step = len(train_loader)\n","\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n","\n","num_classes = 102\n","num_epochs = 2\n","print_every = 5\n","total_step = len(train_loader)\n","\n","config = {\n","    \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n","    \"weight_decay\": tune.loguniform(1e-5, 1e-2),\n","    \"layers\": tune.sample_from(lambda _: [np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)])\n","}\n","\n","scheduler = ASHAScheduler(\n","    metric=\"loss\",\n","    mode=\"min\",\n","    max_t=num_epochs,\n","    grace_period=1,\n","    reduction_factor=2\n",")\n","\n","reporter = CLIReporter(\n","    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxwREC8j45Ff","outputId":"b97e733a-493c-459b-8f3a-6e7f65a48414"},"outputs":[{"name":"stdout","output_type":"stream","text":["11230758\n"]}],"source":["def count_parameters(model)->int:\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(count_parameters(model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtytZDJr45Fg"},"outputs":[],"source":["import requests\n","def post_discord(epoch,train_loss, val_accuracy):\n","    data = {\n","        \"username\": \"Flower Classifier\"\n","    }\n","    data[\"embeds\"] = [\n","        {\n","            \"title\": \"Epoch {} Results\".format(epoch),\n","            \"color\": 0x00ff00,\n","            \"fields\": [\n","                {\n","                    \"name\": \"Training Loss\",\n","                    \"value\": f\"{train_loss}\",\n","                    \"inline\": False\n","                },\n","                {\n","                    \"name\": \"Validation Accuracy\",\n","                    \"value\": f\"{val_accuracy}\",\n","                    \"inline\": False\n","                },\n","            ]\n","        }\n","    ]\n","    r = requests.post(\"https://discord.com/api/webhooks/1052587856343867392/SnR2U4HIdeF6ShECr0k1Mit6yrl3HgjtCMk_LykGD8eQ1qsZViY8HLeYsoBUXOHYfbgP\", json=data)\n","    print(r.status_code)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgCHhSLD45Fh"},"outputs":[],"source":["import gc\n","\n","def train(model, config):\n","    if torch.cuda.is_available():\n","      if torch.cuda.device_count() > 1:\n","        model = nn.DataParallel(model)\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9, weight_decay=config[\"weight_decay\"])\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","        \n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n","        \n","        if epoch % print_every == 0:\n","            model.eval()  # Set the model to evaluation mode\n","            with torch.no_grad():\n","                correct = 0\n","                total = 0\n","                correctT = 0\n","                totalT = 0\n","                for images, labels in valid_loader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted==labels).sum().item()\n","                for images, labels in train_loader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    totalT += labels.size(0)\n","                    correctT += (predicted==labels).sum().item()\n","\n","                print('Training Accuracy accross {} images: {} %'.format(totalT, 100 * correctT / totalT))\n","                print('Validation Accuracy accross {} images: {} %'.format(total, 100 * correct / total))\n","                #post_discord(epoch+1, '{:.4f}'.format(loss.item()), 100 * correct / total)\n","        val_loss = 0.0\n","        val_steps = 0\n","        total = 0\n","        correct = 0\n","        if epoch % 100 == 0 and epoch != 0:\n","            with torch.no_grad():\n","                for images, labels in test_loader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","                    loss = criterion(outputs, labels)\n","                    val_loss += loss.cpu().numpy()\n","                    val_steps += 1\n","                    del images, labels, outputs\n","            torch.save(model.state_dict(), 'model-{}.pth'.format(100 * correct / total))\n","        #tune.report(loss=(val_loss / val_steps), accuracy=correct/total)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gj_kMo1745Fi"},"outputs":[],"source":["torch.save(best.state_dict(), 'hey.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9rNSL2I45Fi"},"outputs":[],"source":["def test(model):\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            del images, labels, outputs\n","\n","        print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))   \n","        return correct / total\n"]},{"cell_type":"code","source":["bestModel = {\"learning_rate\": 0,\n","             \"weight_decay\": 0,\n","}\n","bestAccuracy = 0\n","\n","for trials in range (10):\n","    num_epochs = 40\n","    config = {\n","        \"learning_rate\": np.power(10, np.random.uniform(-5, -3)),\n","        \"weight_decay\": np.power(10, np.random.uniform(-5, -3)),\n","    }\n","    print(config)\n","    model = Rn(Block, [2,2,2,2], num_classes).to(device)\n","    train(model, config)\n","    acc = test(model)\n","    if acc > bestAccuracy:\n","        bestModel = config\n","        bestAccuracy = acc\n","\n","print(bestModel)\n","num_epochs = 300\n","model = Rn(Block, [2,2,2,2], num_classes).to(device)\n","train(model, bestModel)\n","test(model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZhyEck__QmZ","outputId":"4fcfc553-f216-4569-825f-2fdda1853827"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'learning_rate': 0.0005565070811387906, 'weight_decay': 0.00034446768277040396}\n","Epoch [1/40], Loss: 4.6256\n","Training Accuracy accross 1020 images: 5.980392156862745 %\n","Validation Accuracy accross 1020 images: 4.411764705882353 %\n","Epoch [2/40], Loss: 4.2310\n","Epoch [3/40], Loss: 3.8499\n","Epoch [4/40], Loss: 3.8865\n","Epoch [5/40], Loss: 4.3917\n","Epoch [6/40], Loss: 5.4338\n","Training Accuracy accross 1020 images: 15.490196078431373 %\n","Validation Accuracy accross 1020 images: 12.352941176470589 %\n","Epoch [7/40], Loss: 3.8357\n","Epoch [8/40], Loss: 3.6685\n","Epoch [9/40], Loss: 4.5029\n","Epoch [10/40], Loss: 4.7900\n","Epoch [11/40], Loss: 4.1609\n","Training Accuracy accross 1020 images: 23.823529411764707 %\n","Validation Accuracy accross 1020 images: 19.313725490196077 %\n","Epoch [12/40], Loss: 3.9213\n","Epoch [13/40], Loss: 3.4565\n","Epoch [14/40], Loss: 4.3930\n","Epoch [15/40], Loss: 3.7141\n","Epoch [16/40], Loss: 4.0069\n","Training Accuracy accross 1020 images: 26.07843137254902 %\n","Validation Accuracy accross 1020 images: 21.568627450980394 %\n","Epoch [17/40], Loss: 3.6487\n","Epoch [18/40], Loss: 3.7206\n","Epoch [19/40], Loss: 2.1557\n","Epoch [20/40], Loss: 2.1387\n","Epoch [21/40], Loss: 2.3989\n","Training Accuracy accross 1020 images: 30.686274509803923 %\n","Validation Accuracy accross 1020 images: 21.07843137254902 %\n","Epoch [22/40], Loss: 2.5598\n","Epoch [23/40], Loss: 2.7990\n","Epoch [24/40], Loss: 2.4094\n","Epoch [25/40], Loss: 2.8837\n","Epoch [26/40], Loss: 2.8531\n","Training Accuracy accross 1020 images: 39.6078431372549 %\n","Validation Accuracy accross 1020 images: 27.058823529411764 %\n","Epoch [27/40], Loss: 3.5551\n","Epoch [28/40], Loss: 1.1884\n","Epoch [29/40], Loss: 2.3452\n","Epoch [30/40], Loss: 4.2515\n","Epoch [31/40], Loss: 1.6240\n","Training Accuracy accross 1020 images: 44.21568627450981 %\n","Validation Accuracy accross 1020 images: 31.96078431372549 %\n","Epoch [32/40], Loss: 3.1772\n","Epoch [33/40], Loss: 5.6645\n","Epoch [34/40], Loss: 1.8772\n","Epoch [35/40], Loss: 1.6085\n","Epoch [36/40], Loss: 2.8721\n","Training Accuracy accross 1020 images: 45.98039215686274 %\n","Validation Accuracy accross 1020 images: 30.784313725490197 %\n","Epoch [37/40], Loss: 3.0637\n","Epoch [38/40], Loss: 1.9034\n","Epoch [39/40], Loss: 2.8519\n","Epoch [40/40], Loss: 1.9391\n","Accuracy of the network on the 6149 test images: 26.313221662058872 %\n","{'learning_rate': 0.0005078840390207057, 'weight_decay': 8.719351484959092e-05}\n","Epoch [1/40], Loss: 4.4603\n","Training Accuracy accross 1020 images: 5.882352941176471 %\n","Validation Accuracy accross 1020 images: 6.078431372549019 %\n","Epoch [2/40], Loss: 4.2662\n","Epoch [3/40], Loss: 3.5516\n","Epoch [4/40], Loss: 4.0738\n","Epoch [5/40], Loss: 2.6794\n","Epoch [6/40], Loss: 4.2074\n","Training Accuracy accross 1020 images: 17.352941176470587 %\n","Validation Accuracy accross 1020 images: 13.92156862745098 %\n","Epoch [7/40], Loss: 4.5053\n","Epoch [8/40], Loss: 4.2497\n","Epoch [9/40], Loss: 4.7066\n","Epoch [10/40], Loss: 3.9293\n","Epoch [11/40], Loss: 3.9832\n","Training Accuracy accross 1020 images: 22.058823529411764 %\n","Validation Accuracy accross 1020 images: 17.941176470588236 %\n","Epoch [12/40], Loss: 2.6145\n","Epoch [13/40], Loss: 3.8010\n","Epoch [14/40], Loss: 3.2560\n","Epoch [15/40], Loss: 2.5154\n","Epoch [16/40], Loss: 3.7918\n","Training Accuracy accross 1020 images: 22.647058823529413 %\n","Validation Accuracy accross 1020 images: 17.84313725490196 %\n","Epoch [17/40], Loss: 2.8461\n","Epoch [18/40], Loss: 2.6318\n","Epoch [19/40], Loss: 3.5542\n","Epoch [20/40], Loss: 2.5662\n","Epoch [21/40], Loss: 2.8097\n","Training Accuracy accross 1020 images: 26.274509803921568 %\n","Validation Accuracy accross 1020 images: 16.470588235294116 %\n","Epoch [22/40], Loss: 2.5322\n","Epoch [23/40], Loss: 2.9338\n","Epoch [24/40], Loss: 2.1136\n","Epoch [25/40], Loss: 2.7053\n","Epoch [26/40], Loss: 2.5002\n","Training Accuracy accross 1020 images: 33.431372549019606 %\n","Validation Accuracy accross 1020 images: 26.470588235294116 %\n","Epoch [27/40], Loss: 2.2953\n","Epoch [28/40], Loss: 2.2198\n","Epoch [29/40], Loss: 2.7107\n","Epoch [30/40], Loss: 1.5702\n","Epoch [31/40], Loss: 1.8754\n","Training Accuracy accross 1020 images: 42.549019607843135 %\n","Validation Accuracy accross 1020 images: 31.764705882352942 %\n","Epoch [32/40], Loss: 2.3996\n","Epoch [33/40], Loss: 1.9058\n","Epoch [34/40], Loss: 2.2645\n","Epoch [35/40], Loss: 1.8694\n","Epoch [36/40], Loss: 2.8982\n","Training Accuracy accross 1020 images: 48.529411764705884 %\n","Validation Accuracy accross 1020 images: 33.92156862745098 %\n","Epoch [37/40], Loss: 2.3023\n","Epoch [38/40], Loss: 1.9097\n","Epoch [39/40], Loss: 1.4541\n","Epoch [40/40], Loss: 2.4362\n","Accuracy of the network on the 6149 test images: 26.752317449991867 %\n","{'learning_rate': 0.0002676311334963157, 'weight_decay': 4.3384558813547285e-05}\n","Epoch [1/40], Loss: 4.4698\n","Training Accuracy accross 1020 images: 6.2745098039215685 %\n","Validation Accuracy accross 1020 images: 5.294117647058823 %\n","Epoch [2/40], Loss: 3.5446\n","Epoch [3/40], Loss: 4.3163\n","Epoch [4/40], Loss: 4.1782\n","Epoch [5/40], Loss: 3.2440\n","Epoch [6/40], Loss: 3.6043\n","Training Accuracy accross 1020 images: 16.274509803921568 %\n","Validation Accuracy accross 1020 images: 11.764705882352942 %\n","Epoch [7/40], Loss: 3.9674\n","Epoch [8/40], Loss: 2.7525\n","Epoch [9/40], Loss: 2.8487\n","Epoch [10/40], Loss: 3.9368\n","Epoch [11/40], Loss: 3.7470\n","Training Accuracy accross 1020 images: 20.098039215686274 %\n","Validation Accuracy accross 1020 images: 18.431372549019606 %\n","Epoch [12/40], Loss: 2.9634\n","Epoch [13/40], Loss: 3.3473\n","Epoch [14/40], Loss: 3.3276\n","Epoch [15/40], Loss: 3.3777\n","Epoch [16/40], Loss: 4.2895\n","Training Accuracy accross 1020 images: 30.392156862745097 %\n","Validation Accuracy accross 1020 images: 23.725490196078432 %\n","Epoch [17/40], Loss: 3.2998\n","Epoch [18/40], Loss: 3.2971\n","Epoch [19/40], Loss: 2.8738\n","Epoch [20/40], Loss: 1.8020\n","Epoch [21/40], Loss: 2.6777\n","Training Accuracy accross 1020 images: 38.72549019607843 %\n","Validation Accuracy accross 1020 images: 25.686274509803923 %\n","Epoch [22/40], Loss: 4.6316\n","Epoch [23/40], Loss: 3.1740\n","Epoch [24/40], Loss: 3.0845\n","Epoch [25/40], Loss: 4.6862\n","Epoch [26/40], Loss: 1.9318\n","Training Accuracy accross 1020 images: 37.254901960784316 %\n","Validation Accuracy accross 1020 images: 27.45098039215686 %\n","Epoch [27/40], Loss: 3.7374\n","Epoch [28/40], Loss: 1.9868\n","Epoch [29/40], Loss: 1.9157\n","Epoch [30/40], Loss: 2.6367\n","Epoch [31/40], Loss: 3.8379\n","Training Accuracy accross 1020 images: 42.450980392156865 %\n","Validation Accuracy accross 1020 images: 30.0 %\n","Epoch [32/40], Loss: 1.4707\n","Epoch [33/40], Loss: 3.0475\n","Epoch [34/40], Loss: 2.4574\n","Epoch [35/40], Loss: 3.1512\n","Epoch [36/40], Loss: 1.8180\n","Training Accuracy accross 1020 images: 47.05882352941177 %\n","Validation Accuracy accross 1020 images: 32.84313725490196 %\n","Epoch [37/40], Loss: 1.7359\n","Epoch [38/40], Loss: 2.6575\n","Epoch [39/40], Loss: 3.1808\n","Epoch [40/40], Loss: 1.7524\n","Accuracy of the network on the 6149 test images: 28.232232883395675 %\n","{'learning_rate': 1.2426434518256788e-05, 'weight_decay': 2.9160852871875247e-05}\n","Epoch [1/40], Loss: 4.6133\n","Training Accuracy accross 1020 images: 0.6862745098039216 %\n","Validation Accuracy accross 1020 images: 0.8823529411764706 %\n","Epoch [2/40], Loss: 4.7819\n","Epoch [3/40], Loss: 5.0555\n","Epoch [4/40], Loss: 4.5210\n","Epoch [5/40], Loss: 4.6375\n","Epoch [6/40], Loss: 4.5403\n","Training Accuracy accross 1020 images: 2.7450980392156863 %\n","Validation Accuracy accross 1020 images: 2.6470588235294117 %\n","Epoch [7/40], Loss: 4.5352\n","Epoch [8/40], Loss: 4.6836\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lur84xbu45Fj","outputId":"928f96dc-fb3e-433c-920a-8b3758711c0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["11230758\n"]}],"source":["def count_parameters(model)->int:\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(count_parameters(model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0s7aSfm45Fj","outputId":"abe1c151-e861-42f4-baaa-a23e49d89672"},"outputs":[{"name":"stdout","output_type":"stream","text":["Rn(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer0): Sequential(\n","    (0): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","  )\n","  (layer1): Sequential(\n","    (0): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (conv1): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=102, bias=True)\n",")\n"]}],"source":["print(model)"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}